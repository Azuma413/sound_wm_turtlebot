{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# tensorflowがgpuを使っているか確認する\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題点\n",
    "ロボットの現在の方向が分からなければ学習ができないのでは？\n",
    "もしくは操作角度を相対的なものではなく，絶対的なものとするなど。\n",
    "同じ行動の繰り返しに対して罰則を与えるべきでは？\n",
    "軌跡を描画する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-26 01:20:35</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:12.55        </td></tr>\n",
       "<tr><td>Memory:      </td><td>29.9/63.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 10.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env2_5b687_00000</td><td>TERMINATED</td><td>127.0.0.1:39116</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         170.738</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 2.36994</td><td style=\"text-align: right;\">             15.4645</td><td style=\"text-align: right;\">            -7.51627</td><td style=\"text-align: right;\">             13.75</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 01:17:22,901\tINFO wandb.py:307 -- Already logged into W&B.\n",
      "2024-02-26 01:20:35,407\tWARNING util.py:202 -- The `callbacks.on_trial_result` operation took 5.233 s, which may be a performance bottleneck.\n",
      "2024-02-26 01:20:35,409\tWARNING util.py:202 -- The `process_trial_result` operation took 5.236 s, which may be a performance bottleneck.\n",
      "2024-02-26 01:20:35,410\tWARNING util.py:202 -- Processing trial results took 5.237 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2024-02-26 01:20:35,410\tWARNING util.py:202 -- The `process_trial_result` operation took 5.237 s, which may be a performance bottleneck.\n",
      "2024-02-26 01:20:39,852\tINFO tune.py:1042 -- Total run time: 196.96 seconds (192.54 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dreamerv3 import DreamerV3Config\n",
    "from envs.gym_env import MyEnv, MyEnv2\n",
    "from ray import train, tune, air\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.impala import ImpalaConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.replay_buffers.replay_buffer import StorageUnit\n",
    "import os\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import pprint\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return MyEnv()\n",
    "\n",
    "def env_creator2(env_config):\n",
    "    return MyEnv2()\n",
    "\n",
    "tune.register_env(\"my_env\", env_creator)\n",
    "tune.register_env(\"my_env2\", env_creator2)\n",
    "\n",
    "# config = (  # 1. Configure the algorithm,\n",
    "#     DreamerV3Config()\n",
    "#     .environment('my_env')\n",
    "#     .training(\n",
    "#         model_size=\"XS\", # XS, S, M, L, XL\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# https://docs.ray.io/en/latest/rllib/rllib-training.html#specifying-resources\n",
    "\n",
    "# 64x64x3の画像を入力とする場合の設定\n",
    "# model_dict = {\"conv_filters\": [\n",
    "#             [16, [8, 8], 4],\n",
    "#             [32, [4, 4], 4],\n",
    "#             [64, [4, 4], 2],\n",
    "#             ],\n",
    "#             #   \"use_attention\": True,\n",
    "#             #   \"use_lstm\": False,\n",
    "#               }\n",
    "# 128x128x3の画像を入力とする場合の設定\n",
    "model_dict = {\"conv_filters\": [\n",
    "            [16, [8, 8], 4],\n",
    "            [32, [4, 4], 3],\n",
    "            [64, [3, 3], 2],\n",
    "            [256, [6, 6], 1],\n",
    "            ],}\n",
    "\n",
    "replay_buffer_config = {\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        # Although not necessary, we can modify the default constructor args of\n",
    "        # the replay buffer here\n",
    "        \"prioritized_replay_alpha\": 0.5,\n",
    "        \"storage_unit\": StorageUnit.SEQUENCES,\n",
    "        \"replay_burn_in\": 20,\n",
    "        \"zero_init_states\": True,\n",
    "    }\n",
    "exploration_config = {\"type\": \"PerWorkerEpsilonGreedy\",}\n",
    "\n",
    "# config = ImpalaConfig().environment(env=\"my_env2\", disable_env_checking=True, render_env=True).rollouts(num_rollout_workers=9).training(model=model_dict).framework(framework=\"torch\").resources(num_gpus=1)\n",
    "config = PPOConfig().environment(env=\"my_env2\", disable_env_checking=True, render_env=True).rollouts(num_rollout_workers=9).training(train_batch_size=256, model=model_dict).framework(framework=\"torch\").resources(num_gpus=1)\n",
    "# config = SACConfig().environment(env=MyEnv2, disable_env_checking=True, render_env=True).rollouts(num_rollout_workers=9).training(model=model_dict).framework(framework=\"torch\").resources(num_gpus=1).exploration(exploration_config=exploration_config)\n",
    "# config = SACConfig().environment(\"CartPole-v1\", disable_env_checking=True, render_env=True).rollouts(num_rollout_workers=9).training().framework(framework=\"torch\").resources(num_gpus=1).exploration(exploration_config=exploration_config)\n",
    "# config = DreamerV3Config().environment(env=\"my_env2\", disable_env_checking=True, render_env=True).training(model=model_dict).resources(num_gpus=1)\n",
    "# Dreamerは複数Agentに対応していない\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",  # 2. Specify the algorithm name\n",
    "    run_config=train.RunConfig(\n",
    "        stop={#\"episode_reward_mean\": 20\n",
    "              \"timesteps_total\": 10,\n",
    "              },  # 学習の終了条件\n",
    "        callbacks=[WandbLoggerCallback(project=\"Wandb_nearme\")]  # コメントアウトを外すとwandbにログが送信される(https://docs.wandb.ai/quickstart)\n",
    "    ),\n",
    "    param_space=config\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "c:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "c:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best performing trial's final reported metrics:\n",
      "\n",
      "{'episode_len_mean': 9.571428571428571,\n",
      " 'episode_reward_max': 16.619508763643715,\n",
      " 'episode_reward_mean': 1.3438241469634444,\n",
      " 'episode_reward_min': -4.810336984042478}\n",
      "come here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=29584)\u001b[0m 2024-02-26 00:27:00,336\tWARNING env_runner_v2.py:297 -- Could not import gymnasium.envs.classic_control.rendering! Try `pip install gymnasium[all]`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 00:27:01,382\tINFO trainable.py:164 -- Trainable.setup took 10.266 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-02-26 00:27:01,385\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n",
      "c:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come here\n",
      "come here\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[0;32m     25\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 26\u001b[0m     action, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:559\u001b[0m, in \u001b[0;36mPolicy.compute_single_action\u001b[1;34m(self, obs, state, prev_action, prev_reward, info, input_dict, episode, explore, timestep, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m [episode]\n\u001b[1;32m--> 559\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_actions_from_input_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSampleBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# Some policies don't return a tuple, but always just a single action.\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# E.g. ES and ARS.\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:572\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_actions_from_input_dict\u001b[1;34m(self, input_dict, explore, timestep, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_batches:\n\u001b[0;32m    566\u001b[0m     seq_lens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    567\u001b[0m         [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(state_batches[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m    568\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,\n\u001b[0;32m    569\u001b[0m         device\u001b[38;5;241m=\u001b[39mstate_batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    570\u001b[0m     )\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_action_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\utils\\threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\policy\\torch_policy_v2.py:1293\u001b[0m, in \u001b[0;36mTorchPolicyV2._compute_action_helper\u001b[1;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     dist_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_class\n\u001b[1;32m-> 1293\u001b[0m     dist_inputs, state_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(dist_class, functools\u001b[38;5;241m.\u001b[39mpartial)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dist_class, TorchDistributionWrapper)\n\u001b[0;32m   1298\u001b[0m ):\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dist_class` (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) not a TorchDistributionWrapper \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclass! Make sure your `action_distribution_fn` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`make_model_and_action_dist` return a correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dist_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1304\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\modelv2.py:263\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[1;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[0;32m    260\u001b[0m         restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext():\n\u001b[1;32m--> 263\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[0;32m    266\u001b[0m     input_dict\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m=\u001b[39m restored\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\alaleh\\anaconda3\\envs\\rl_env\\lib\\site-packages\\ray\\rllib\\models\\torch\\visionnet.py:243\u001b[0m, in \u001b[0;36mVisionNetwork.forward\u001b[1;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Permuate b/c data comes in as [B, dim, dim, channels]:\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Store features to save forward pass when getting value_function out.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result()\n",
    "print(\"\\nBest performing trial's final reported metrics:\\n\")\n",
    "\n",
    "metrics_to_print = [\n",
    "    \"episode_reward_mean\",\n",
    "    \"episode_reward_max\",\n",
    "    \"episode_reward_min\",\n",
    "    \"episode_len_mean\",\n",
    "]\n",
    "pprint.pprint({k: v for k, v in best_result.metrics.items() if k in metrics_to_print})\n",
    "\n",
    "print(\"come here\")\n",
    "loaded_algo = Algorithm.from_checkpoint(best_result.checkpoint)\n",
    "print(\"come here\")\n",
    "policy = loaded_algo.get_policy()\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# env = MyEnv()\n",
    "\n",
    "episode_reward = 0\n",
    "terminated = truncated = False\n",
    "obs, info = env.reset()\n",
    "print(\"come here\")\n",
    "while not terminated and not truncated:\n",
    "    env.render()\n",
    "    action, _, _ = policy.compute_single_action(obs)\n",
    "    print(f\"action :{action}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"reward :{reward}\")\n",
    "    print(f\"terminated :{terminated}\")\n",
    "    print(f\"truncated :{truncated}\")\n",
    "    episode_reward += reward\n",
    "print(f\"Episode reward :{episode_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
